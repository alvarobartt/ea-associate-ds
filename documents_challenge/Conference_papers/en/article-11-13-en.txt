Results We compared the performance of our translation engine when trained on four different bitexts. Contrary to our former experiment, cosine and edit performed similarly well. This could be explained by the shorter length of the documents and by the filtering step applied on the parallel pairs, which seems to eliminate untrusted pairs. Inspection of their bitexts showed that they shared only 229 document pairs, so we trained another translation engine on the union of those bitexts. The scores of all the translation engines are reported in Table 2.