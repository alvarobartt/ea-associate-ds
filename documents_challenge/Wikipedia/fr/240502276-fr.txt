En statistique et probabilité, la variance est une mesure arbitraire servant à caractériser la dispersion d'une distribution ou d'un échantillon.DéfinitionSi X est une variable aléatoire réelle :(X)\equiv V(X) \equiv \mathbb((X-\mathbb(X))^2)\mathbb'' étant l'espérance mathématiqueOn peut interpréter la variance comme la moyenne des carrés des écarts à la moyenne (rigoureusement: l'espérance des carrés des écarts à l'espérance). Elle permet de caractériser la dispersion des valeurs par rapport à la moyenne. Ainsi, une distribution avec une même espérance et une variance plus grande apparaîtra comme plus étalée. Le fait que l'on prenne le carré de ces écarts à la moyenne évite que des écarts positifs et négatifs ne s'annulent. (X)\equiv \sigma^2_X PropriétésLa variance est toujours positive ou nulle.Lorsque la variance est nulle, cela signifie que la variable aléatoire correspond à une constante (toutes les réalisations sont identiques).Formule alternative de calcul de la variance:(X)=\mathbb(X^2)-\mathbb(X)^2&lt;/math&gt;Cette formule s'énonce ainsi que la variance est égale à l'espérance du carré de X moins le carré de l'espérance de X. La formule permet de calculer parfois plus simplement la variance que d'après la définition.Sa démonstration est faite dans le théorème de König-Huyghens.Variance d'une transformation linéaire:(aX+b)=a^2\operatorname(X)&lt;/math&gt;(aX + b)= a \operatorname(X) + b\, On a alors:\begin\operatorname(aX+b)& = E-E[aX+b)^2] \qquad \text\\ & = E-aE[X-b)^2] \\& = E-aE[X)^2]\\& = E-E[X)^2]\\& = a^2E-E[X)^2] \\&= a^2\operatorname(X) \end  On remarque à travers cette propriété que le fait de déplacer simplement une distribution (ajouter +b) ne modifie pas sa variance. Par contre, changer l'échelle (multiplier par a) modifie la variance quadratiquement. Cette propriété permet également de confirmer la remarque établie précédemment que la variance d'une constante est nulle, en effet, \operatorname(b)= 0.Variance de deux variablesSi \operatorname(X,Y) désigne la covariance des variables aléatoires X et Y, alors:(X+Y) = \operatorname(X) + \operatorname(Y) + 2\operatorname(X,Y)Variance de deux variables indépendantes(X+Y) = \operatorname(X) + \operatorname(Y)Il faut faire attention au fait que \operatorname(X-Y) = \operatorname(X) + \operatorname(Y)! Même si les variables sont soustraites, leur variances s'additionnent.Variance de la moyenne de variables indépendantes et de variance égaleEn définissant \overline=\frac\sum_^n X_i (\overline) = \frac \cdot \operatorname(X)(\overline)=\operatorname\left(\frac\sum_^n X_i\right)=\frac\operatorname\left(\sum_^n X_i\right) = \frac  n \operatorname(X) = \frac (X) Ecart typeL'écart type est la racine carrée de la variance:\sigma_x = \sqrtCas discretLa variance V(X) représente la moyenne des carrés des écarts à la moyenne : elle permet de caractériser, tout comme l'écart type, la dispersion des valeurs x_i par rapport à la moyenne, notée \overline , ou encore E(X).Soit une série statistique (x_i, n_i)_ de moyenne \overline et d'effectif total n (c’est-à-dire n=\sum_^k n_i et p_i=\frac).La variance de cette série est alors :V(X)=\sum_^k p_i(x_i-\overline)^2SimplificationLa moyenne peut être considérée comme le barycentre de la série.D'après le théorème de König , on a : V(X)=\sum_^kp_i(x_i^2)-\overline^2^k p_i(x_i-\overline)^2V(X)=\sum_^k p_i(x_i^2-2x_i\overline+\overline^2)V(X)=\sum_^k p_ix_i^2-2\overline\sum_^k p_ix_i+\sum_^k p_i\overline^2V(X)=\sum_^k p_ix_i^2-2\overline\sum_^k p_ix_i+\overline^2\sum_^k p_i Or, \sum_^k p_i=1, et \sum_^k p_ix_i=\overline, donc on a:V(X)=\sum_^k p_ix_i^2-\overline^2ÉquiprobabilitéDans le cas d'équiprobabilité,V(X) = \frac1n\sum_^n(x_i-\bar x)^2 = \frac1n\sum_^n x_i^2 - \bar x^2Remarque: égalité toujours vraie, même s'il n'y a pas équiprobabilité! (cf développer le calcul et sortir la moyenne de la somme dans le terme croisé)Cas continuDans le cas continu, la variance se calcule de la façon suivante :V(X)= \int_\mathbb R x^2 \cdot f(x) \cdot \mathrm dx - \left( \int_\mathbb R x \cdot f(x) \cdot \mathrm dx \right)^2Variance d'un vecteur aléatoireSi l'on définit X_ comme un vecteur aléatoire qui comporte k variables et  \Mu  comme le vecteur des k espérances de X, on définit alors la variance comme: \equiv \operatorname1\equiv \mathbb\left1-\Mu)(X_-\Mu)'\rightIl s'agit alors d'une matrice carrée de dimension k, appelée matrice de variance-covariance, qui comporte sur sa diagonale les variances dechaque élément du vecteur et en dehors de la diagonale les covariances. Cette matrice est symétrique et définie positive.On a les propriétés suivantes:kX_=V\operatornameXV'EstimationDeux estimateurs sont généralement utilisés pour la variance:s_n^2 \equiv \hat\sigma ^2= \frac 1n \sum_^n \left(y_i - \overline \right)^ 2 = \left(\frac \sum_^y_i^2\right) - \overline^2,ets^2_ \equiv \hat\sigma ^2= \frac \sum_^n\left(y_i - \overline \right)^ 2 = \frac\sum_^n y_i^2 - \frac \overline^2,PropriétésBiaisL'estimateur s^2_, est biaisé:  E(s^2_)=\frac \sigma^2 est:\begins^2_ &\equiv\frac \sum_^n \left( x_i - \overline\right)^2 \\ & = \left(\frac\sum_^n x_i^2\right) - \overline^2 \end . La deuxième égalité s'obtient d'après le Théorème de König-Huyghens.Nous allons calculer l'espérance de l'estimateur d'après la deuxième formule:E(s^2_) = E\left(\frac\sum_^n x_i^2\right) - E(\overline^2).Il faut donc étudier l'espérance des deux termes, on verra que:E\left(\frac\sum_^n x_i^2\right)=E(X)^2+V(X)\sum_^n x_i^2\right)=\fracE(\sum_^n x_i^2)=\frac\sum_^n E(x_i^2)=\fracn E(x_i^2)=E(x_i^2).On a supposé que tous les réalisations ont la même espérance: E(x_i)=E(X)  En appliquant de nouveau la formule de König-Huyghens: E(x_i^2)=E(X^2)= E(X)^2 +V(X). E(\overline^2)=E(X)^2+\fracV(X)=\frac\sum_^n x_i de l'échantillon est une variable aléatoire (si on change les individus alors \overline varie):d'espérance  E(\overline) = E(X) de variance: V(\overline)=\frac \cdot V(X) (la moyenne de n variables aléatoires fluctue moins qu'une seule variable aléatoire)En appliquant de nouveau la formule de König-Huyghens:E(\overline^2)=E(\overline)^2+V(\overline)=E(X)^2+\fracV(X).On a donc E(s^2_) = E(X)^2+V(X) - E(X)^2-\fracV(X)=\fracV(X).La variance s'' de l'échantillon fluctue donc autour de \fracV(X) et non autour de ''V(X) comme on aurait pu s'y attendre.L'estimateur s^2_, est un estimateur sans biais. par \frac pour avoir un estimateur sans biais: E\lefts^2_\right= \frac Es^2_=\frac \frac\sigma^2=\sigma^2Pourquoi n-1?Le fait que l'estimateur de la variance doive être divisé par n-1 (et donc dans un certain sens moins précis) pour être sans biais provient du fait que l'estimation de la variance implique l'estimation d'un paramètre en plus, l'espérance. Cette correction tient compte donc du fait que l'estimation de l'espérance induit une incertitude de plus. En effet:  est sans biais  lorsque l'espérance est inconnue, on avait montré que: E(s^2_) = E\left(\frac\sum_^n x_i^2\right) - E(\overline^2). Puis calculé que:E\left(\frac\sum_^n x_i^2\right)=E(X)^2+V(X) E(\overline^2)=E(X)^2+\fracV(X)Cependant, le deuxième calcul est désormais différent: EX étant connu, on pose que  EX=\mu et on a:  E\mu^2=E\mu^2  Donc on a que: E(\overline^2)=EX^2.La formule devient alors: E(s^2_) = E(X)^2+V(X)- E(X)^2=V(X)ConvergenceLes estimateurs s^2_ et s^2_ sont convergents en probabilité.  et s^2_ \quad \xrightarrow \quad \sigma^2  si les observations sont iid (\mu, \sigma^2). ^n \left(y_i - \overline \right)^ 2=\left(\frac \sum_^y_i^2\right) - \overline^2Et étudions la convergence des termes séparément:\overline^2 \xrightarrow \quad \mu^2  par le .\frac 1n \sum_^n \left(y_i - \overline \right)^ 2 \xrightarrow \quad \operatornamex^2= \mu^2+\sigma^2 par la loi des grands nombres.Alors s_n^2 \quad \xrightarrow \quad \mu^2+\sigma^2-\mu^2 = \sigma^2Comme ce résultat est asymptotique, il s'applique également à s^2_, qui est asymptotiquement équivalent à s^2_ Distribution des estimateursEn tant que fonction de variables aléatoires, l'estimateur de la variance est également une variable aléatoire. Sous l'hypothèse que les y_i sont des observations indépendantes d'une loi normale, le  montre que s^2_ suit une loi du χ²:(n-1)\frac\sim\chi^2_.  En conséquence, il suit que  \operatorname(s^2_)=\sigma^2.. Cette propriété d'absence de biais peut cependant être démontrée même sans l'hypothèse de normalité des observations.Méthodes de calculLe calcul par ordinateur de la variance empirique peut poser certains problèmes, notamment à cause de la somme des carrés. La page anglaise: Algorithms for calculating variance décrit le problème ainsi que des algorithmes proposés.Voir aussiÉcart typeCovariancemesures secondairesCatégorie:Statistique descriptive Catégorie:Probabilitésar:تباين ca:Variància cs:Rozptyl (statistika) da:Varians de:Varianz el:Διακύμανση en:Variance eo:Varianco es:Varianza et:Dispersioon eu:Bariantza fa:واریانس fi:Varianssi gl:Varianza he:שונות id:Varians is:Dreifni it:Varianza ja:分散 ko:분산 lt:Dispersija mk:Варијанса nl:Variantie no:Varians pl:Wariancja pt:Variância ru:Дисперсия случайной величины simple:Variance sk:Rozptyl (štatistika) sl:Varianca su:Varian sv:Varians tr:Varyans uk:Дисперсія випадкової величини ur:تفاوت vi:Phương sai zh:方差