{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA Assignment 02 - Data Preprocessing\n",
    "__Authored by: Álvaro Bartolomé del Canto (alvarobartt @ GitHub)__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media-exp1.licdn.com/dms/image/C561BAQFjp6F5hjzDhg/company-background_10000/0?e=2159024400&v=beta&t=OfpXJFCHCqdhcTu7Ud-lediwihm0cANad1Kc_8JcMpA\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start this Jupyter Notebook with a little recap from the previous one named `01 - Data Exploration.ipynb` where we explored the available data and extracted some conclusion and useful details that may be useful during this Jupyter Notebook, so please, check the previous Notebook before proceeding.\n",
    "\n",
    "So on, we will be using the same `Loading Data` and `Cleaning Data` Jupyter cells in order to load the data and clean it (since there were some invalid/duplicated values), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../documents_challenge/Wikipedia',\n",
       " '../documents_challenge/Conference_papers',\n",
       " '../documents_challenge/APR',\n",
       " '../documents_challenge/PAN11']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directories = glob.glob('../documents_challenge/*')\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 423 ms, sys: 117 ms, total: 540 ms\n",
      "Wall time: 541 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for directory in directories:\n",
    "    context = directory.split('/')[-1].lower()\n",
    "    \n",
    "    for subdir in glob.glob(f\"{directory}/*\"):\n",
    "        lang = subdir.split('/')[-1].lower()\n",
    "        \n",
    "        for file in glob.glob(f\"{subdir}/*\"):\n",
    "            data.append({\n",
    "                'lang': lang,\n",
    "                'context': context,\n",
    "                'text': open(file, 'r').read()\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>Watchmen is a twelve-issue comic book limite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The Citigroup Center (formerly Citicorp Cente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>| birth_place = | death_date = | death_place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>Marbod or Maroboduus (born c. in 30 BC, died ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The Sylvester Medal is a bronze medal awarded ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang    context                                               text\n",
       "0   en  wikipedia    Watchmen is a twelve-issue comic book limite...\n",
       "1   en  wikipedia   The Citigroup Center (formerly Citicorp Cente...\n",
       "2   en  wikipedia   | birth_place = | death_date = | death_place ...\n",
       "3   en  wikipedia   Marbod or Maroboduus (born c. in 30 BC, died ...\n",
       "4   en  wikipedia  The Sylvester Medal is a bronze medal awarded ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13651</th>\n",
       "      <td>en</td>\n",
       "      <td>conference_papers</td>\n",
       "      <td>This approach naturally involves an agglomerat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13701</th>\n",
       "      <td>en</td>\n",
       "      <td>conference_papers</td>\n",
       "      <td>We can see that all these proposals have in co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13725</th>\n",
       "      <td>en</td>\n",
       "      <td>conference_papers</td>\n",
       "      <td>Contribution of conceptual vectors to lexical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13752</th>\n",
       "      <td>en</td>\n",
       "      <td>conference_papers</td>\n",
       "      <td>Since version 2, relations as derivationally r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13790</th>\n",
       "      <td>en</td>\n",
       "      <td>conference_papers</td>\n",
       "      <td>This article describes conceptual\\n vectors th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21451</th>\n",
       "      <td>en</td>\n",
       "      <td>pan11</td>\n",
       "      <td>\\n\\nFor Juanita, who had spent all day sewing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22259</th>\n",
       "      <td>es</td>\n",
       "      <td>pan11</td>\n",
       "      <td>La hermosa canción, que canta Margarita mientr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22726</th>\n",
       "      <td>es</td>\n",
       "      <td>pan11</td>\n",
       "      <td>Volvió a los dos meses, muerto de hambre, mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22821</th>\n",
       "      <td>es</td>\n",
       "      <td>pan11</td>\n",
       "      <td>(N. de la E.)\\n\\n[32] Nochebuena chiquita, as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23072</th>\n",
       "      <td>es</td>\n",
       "      <td>pan11</td>\n",
       "      <td>Otro elemento se reitera igualmente en sus no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lang            context  \\\n",
       "13651   en  conference_papers   \n",
       "13701   en  conference_papers   \n",
       "13725   en  conference_papers   \n",
       "13752   en  conference_papers   \n",
       "13790   en  conference_papers   \n",
       "...    ...                ...   \n",
       "21451   en              pan11   \n",
       "22259   es              pan11   \n",
       "22726   es              pan11   \n",
       "22821   es              pan11   \n",
       "23072   es              pan11   \n",
       "\n",
       "                                                    text  \n",
       "13651  This approach naturally involves an agglomerat...  \n",
       "13701  We can see that all these proposals have in co...  \n",
       "13725  Contribution of conceptual vectors to lexical ...  \n",
       "13752  Since version 2, relations as derivationally r...  \n",
       "13790  This article describes conceptual\\n vectors th...  \n",
       "...                                                  ...  \n",
       "21451   \\n\\nFor Juanita, who had spent all day sewing...  \n",
       "22259  La hermosa canción, que canta Margarita mientr...  \n",
       "22726   Volvió a los dos meses, muerto de hambre, mal...  \n",
       "22821   (N. de la E.)\\n\\n[32] Nochebuena chiquita, as...  \n",
       "23072   Otro elemento se reitera igualmente en sus no...  \n",
       "\n",
       "[116 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_data = data[data.duplicated(subset=('text',), keep='first')]\n",
    "duplicated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23012, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(subset=('text',), keep='first', inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23011, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['text'] != 'translation not available']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__At this point we have already loaded and cleaned the data as defined in the previous Jupyter Notebook, so now we can proceed with the NLP Data Preprocessing.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Pre-Processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already stated, the preprocessing is one of the most relevant steps in a NLP pipeline, since when we preprocess text we intend to give additional value to the text, which means that we are enriching our raw data in order to help the model out before we feed it.\n",
    "\n",
    "When it comes to NLP preprocessing there are some common steps since it is usual that the text is not unified into lower case, so it contains both upper and lower characters, a common piece of text contains stopwords such as pronouns, determinants, etc., if the text has been downloaded from Internet it may contain HTML tags, it may also contain multiple spaces or line breaks, etc.\n",
    "\n",
    "So we will just try to cover that in a really generic way first, but then in a more detailed one, since the stopwords are different depending on the language, a language model can be applied for either stemming or lemmatization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Python library `unidecode` for the text cleaning so as to transform any string into a unidecoded one, which in our case, this library will just remove all the accents from both Spanish and French text, and maybe from English texts if there's any (by mistake or maybe because the text mentions words in other languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unidecode example/s presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meme si je travaille chez EA, j'aimerais continuer a etudier\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(\"Même si je travaille chez EA, j'aimerais continuer à étudier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aunque trabaje en EA, me gustaria seguir estudiando'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(\"Aunque trabaje en EA, me gustaría seguir estudiando\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may have seen, the accents have dissappeared but the vowels which contained those accents are still in there, so they have not been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will proceed with defining the regular expresions, which in this case as already mentioned, will be used for: removing the URLs (URL_PATTERN), removing the HTML tags and values (HTML_PATTERN), removing the punctuation signs/marks (PUNCTUATION_PATTERN), to just keep the characters and discard the numbers and any other characters (NUMBER_PATTERN) and to remove multiple spaces (SPACES_PATTERN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "URL_PATTERN = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "HTML_PATTERN = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "PUNCTUATION_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "NUMBER_PATTERN = re.compile(r'[\\d]+')\n",
    "SPACES_PATTERN = re.compile(r'[ ]{2,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions example/s presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ea.com/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_PATTERN.findall(\"Please, visit our web at https://www.ea.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' DELETE Avez-vous déjà joué à Rocket Arena? DELETE '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML_PATTERN.sub(\" DELETE \", \"<b>Avez-vous déjà joué à Rocket Arena?</b>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿', '?', ',', ',', '¿', '?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCTUATION_PATTERN.findall(\"¿Qué tal ha parecido la presentación de EA?, mola, ¿no?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've just spent up to  DELETE  hours playing EA's Harry Potter Quidditch Word Cup game.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_PATTERN.sub(\" DELETE \", \"I've just spent up to 345 hours playing EA's Harry Potter Quidditch Word Cup game.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['        ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPACES_PATTERN.findall(\"Les jeux EA?        simplement le meilleur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already seen, the regular expressions work as expected since they replace, remove, find, etc. the matching parts of the text to the introduced regular expression in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will proceed with the next part, which is the stopword removal part, where we will use a corpus from one of the main NLP libraries for Python which is named NLTK (Natural Language Tool Kit), which contains a corpus of stopwords in multiple languages and, in this case, in the languages we need to solve the problem we are facing to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')\n",
    "french_stopwords = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords removal example/s presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gustaría trabajar ea\n"
     ]
    }
   ],
   "source": [
    "texto = \"me gustaría trabajar en ea\".split()\n",
    "    \n",
    "for palabra in spanish_stopwords:\n",
    "    texto = list(filter((palabra.lower()).__ne__, texto))\n",
    "    \n",
    "print(' '.join(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j'aimerais travailler chez ea\n"
     ]
    }
   ],
   "source": [
    "texte = \"j'aimerais travailler chez ea\".split()\n",
    "\n",
    "for mot in french_stopwords:\n",
    "    texte = list(filter((mot.lower()).__ne__, texte))\n",
    "    \n",
    "print(' '.join(texte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'd love work ea\n"
     ]
    }
   ],
   "source": [
    "text = \"i'd love to work for ea\".split()\n",
    "\n",
    "for word in english_stopwords:\n",
    "    text = list(filter((word.lower()).__ne__, text))\n",
    "    \n",
    "print(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may have seen the stopword removal does not work as well as expected fot both English and French, since those languages tend to use the apostrophe (') to shorten words that appear together, mainly after personal pronouns when they are followed by a verb which starts by a vowel or some other rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, we will define a step in order to solve it, which will split the characters before and after the apostrophe, and the apostrophe will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j aimerais travailler chez ea'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"j'aimerais travailler chez ea\".replace(\"'\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we fixed this issue creating a new rule which should be applied to both English and French texts in order to split the words that contain an apostrphe and removing the apostrophe. Anyway, we will still include this step into the Spanish Preprocessing Pipeline, since some words from other languages may be found so as to ensure that they are all preprocessed the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming (Discarded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will proceed with the Stemming, which is a NLP method to reduce each word to their root, which means that different words with the same root with be transformed to their root so that those words are the same, which can add value to the preprocessing since we have a lot of different words from different language, and this is a way to reduce the size of the input data we will be using to feed the model.\n",
    "\n",
    "In this case, we will also be using NLTK since it contains stemmers for English, Spanish and French, which should cover all our needs for now. So on, we will just transform each word to its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "english_stemmer = SnowballStemmer('english')\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "french_stemmer = SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming example/s presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algun dia trabaj par ea\n"
     ]
    }
   ],
   "source": [
    "texto = \"algun dia trabajaré para ea\".split()\n",
    "resultado = list()\n",
    "\n",
    "for palabra in texto:\n",
    "    resultado.append(spanish_stemmer.stem(palabra))\n",
    "\n",
    "print(' '.join(resultado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "someday i'll be work for ea\n"
     ]
    }
   ],
   "source": [
    "text = \"someday i'll be working for ea\".split()\n",
    "result = list()\n",
    "\n",
    "for word in text:\n",
    "    result.append(english_stemmer.stem(word))\n",
    "\n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un jour je travaill pour ea\n"
     ]
    }
   ],
   "source": [
    "texte = \"un jour je travaillerai pour ea\".split()\n",
    "resultat = list()\n",
    "\n",
    "for mot in texte:\n",
    "    resultat.append(french_stemmer.stem(mot))\n",
    "\n",
    "print(' '.join(resultat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Update__: Stemming will be removed, since we will not know the language in which an input text is written, and the model is supposed to classify any input text in its context regarless the language, so we do not need anything this specific on the preprocessing part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on, in order to conclude, we already defined and tested all the NLP preprocessing steps that we will need to accomplish the task of preprocessing the data so as to feed the model in the next notebook. Anyway, we still need to implement it as some Python interfaces regarding the language, so when data is received, we will just need to apply the defined preprocessing function using the required interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we designed all the NLP Preprocessing pipeline steps we will procceed to its implementation over a random sample text so as to see how it works and in order to initially evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en',\n",
       " 'apr',\n",
       " 'i bought this book because i wanted to have a clear idea on how to build straw \"greb. it is particularly well studied. and makes you want to build straw. well explained and effectively illustrated this book will certainly refer to my future site. the author even provided the essential key to the calculation of structure in the case of small buildings, it is concise but effective.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "sample_lang, sample_context, sample_text = data.iloc[choice(range(len(data)))]\n",
    "sample_lang, sample_context, sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i bought this book because i wanted to have a clear idea on how to build straw \"greb. it is particularly well studied. and makes you want to build straw. well explained and effectively illustrated this book will certainly refer to my future site. the author even provided the essential key to the calculation of structure in the case of small buildings, it is concise but effective.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = sample_text.replace('\\t', ' ').replace('\\n', ' ')\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i bought this book because i wanted to have a clear idea on how to build straw \"greb. it is particularly well studied. and makes you want to build straw. well explained and effectively illustrated this book will certainly refer to my future site. the author even provided the essential key to the calculation of structure in the case of small buildings, it is concise but effective.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = unidecode(sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i bought this book because i wanted to have a clear idea on how to build straw greb it is particularly well studied and makes you want to build straw well explained and effectively illustrated this book will certainly refer to my future site the author even provided the essential key to the calculation of structure in the case of small buildings it is concise but effective '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = (\n",
    "    URL_PATTERN, HTML_PATTERN, PUNCTUATION_PATTERN,\n",
    "    NUMBER_PATTERN, SPACES_PATTERN\n",
    ")\n",
    "\n",
    "for pattern in patterns:\n",
    "    sample_text = pattern.sub(' ', sample_text)\n",
    "    \n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i bought this book because i wanted to have a clear idea on how to build straw greb it is particularly well studied and makes you want to build straw well explained and effectively illustrated this book will certainly refer to my future site the author even provided the essential key to the calculation of structure in the case of small buildings it is concise but effective'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = sample_text.strip().lower()\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i bought this book because i wanted to have a clear idea on how to build straw greb it is particularly well studied and makes you want to build straw well explained and effectively illustrated this book will certainly refer to my future site the author even provided the essential key to the calculation of structure in the case of small buildings it is concise but effective'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = sample_text.replace(\"'\", \" \")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought book wanted clear idea build straw greb particularly well studied makes want build straw well explained effectively illustrated book certainly refer future site author even provided essential key calculation structure case small buildings concise effective'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = english_stopwords if sample_lang == 'en' else spanish_stopwords if sample_lang == 'es' else french_stopwords\n",
    "\n",
    "sample_text = sample_text.split(' ')\n",
    "\n",
    "for word in stopwords:\n",
    "    sample_text = list(filter((word.lower()).__ne__, sample_text))\n",
    "\n",
    "sample_text = ' '.join(sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought book wanted clear idea build straw greb particularly well studied makes want build straw well explained effectively illustrated book certainly refer future site author even provided essential key calculation structure case small buildings concise effective'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = SPACES_PATTERN.sub(' ', sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once the research has been made and the preprocessing pipeline has been tested, we will just proceed with the Python implementation of an Interface in order to create a single preprocessing pipeline to preprocess all the data available in the previously loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATTERNS = (\n",
    "    URL_PATTERN, HTML_PATTERN, PUNCTUATION_PATTERN,\n",
    "    NUMBER_PATTERN, SPACES_PATTERN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: below you can see that there are some additional stopwords, since in the extra version of this Jupyter Notebook that can be found in `research/02 - Extra Data Preprocessing.ipynb` we have applied a TF-IDF Vectorizer over the preprocessed data generated below. So on, this Jupyter Notebook has been run twice, and the extra version just contains additional resources that are already implemented in this Notebook since this is the final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = english_stopwords + spanish_stopwords + french_stopwords\n",
    "\n",
    "ADDITIONAL_STOPWORDS = [\n",
    "    'much', 'despues', 'first', 'categoria', 'aqui', 'thumb', 'also', 'tres', 'asi', \n",
    "    'three', 'one', 'still', 'aquella', 'like', 'aquel', 'mas', 'tal', 'tan', 'hacia', \n",
    "    'went', 'two', 'new', 'even', 'would', 'tras', 'could', 'pues', 'without', 'category', \n",
    "    'many', 'twoone', 'tambien', 'well', 'solo', 'dos'\n",
    "]\n",
    "\n",
    "STOPWORDS += ADDITIONAL_STOPWORDS\n",
    "STOPWORDS = set(list(STOPWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreProcessor(object):\n",
    "    \"\"\"\n",
    "    Custom PreProcessor\n",
    "\n",
    "    Preprocesses the introduced raw text to transform it into clean text. This\n",
    "    preprocessing pipe is regex based.\n",
    "\n",
    "        >>> from apinlp.nlp.preprocessing import CustomPreProcessor\n",
    "        >>> preprocessor = CustomPreProcessor()\n",
    "        >>> print(preprocessor._preprocess(\"Visit us at https://www.ea.com/\"))\n",
    "        \"visit us\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, strip_accents=True):\n",
    "        self.strip_accents = strip_accents\n",
    "        \n",
    "        self.patterns = BASE_PATTERNS\n",
    "        self.additional_patterns = (SPACES_PATTERN,)\n",
    "\n",
    "        self.stopwords = STOPWORDS\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Cleans and applies a preprocessing layer to raw text\"\"\"\n",
    "        text = text.replace('\\t', ' ').replace('\\n', ' ')\n",
    "        \n",
    "        if self.strip_accents:\n",
    "            text = unidecode(text)\n",
    "\n",
    "        for pattern in self.patterns:\n",
    "            text = pattern.sub(' ', text)\n",
    "\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace(\"'\", \" \")\n",
    "        \n",
    "        text = [word for word in text.split(' ') if len(word) > 2]\n",
    "\n",
    "        for word in self.stopwords:\n",
    "            text = list(filter((word.lower()).__ne__, text))\n",
    "\n",
    "        text = ' '.join(text)\n",
    "            \n",
    "        for pattern in self.additional_patterns:\n",
    "            text = pattern.sub(' ', text)\n",
    "    \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = CustomPreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visit'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor._preprocess(text=\"Visit us at https://www.ea.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visitanos'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor._preprocess(text=\"Visítanos en https://www.ea.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visitez'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor._preprocess(text=\"Visitez-nous sur https://www.ea.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Future Note__: additionally this feature may be included in a Python package so as to create a web service to test the models with real unseen data in order to ease its usage via an API instead of requiring to interact with the Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will proceed with a simple overview on the preprocessed data, since we are applying our preprocessing interface named `CustomPreProcessor` to every single text in the dataset regardless the language. So on, this means that we are going to create a new column which will contain the preprocessed text, which we will be using later so as to vectorizer it and feed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 21s, sys: 245 ms, total: 7min 21s\n",
      "Wall time: 7min 21s\n"
     ]
    }
   ],
   "source": [
    "%time data['preprocessed_text'] = data['text'].apply(preprocessor._preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once the preprocessed_text data has been created, we will just drop the original text column since it is not longer useful in this project, since as we already said, we will vectorize and feed the model with the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>context</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>watchmen twelve issue comic book limited serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>citigroup center formerly citicorp center tall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>birth_place death_date death_place party conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>marbod maroboduus born died king marcomanni no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>sylvester medal bronze medal awarded every yea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang    context                                  preprocessed_text\n",
       "0   en  wikipedia  watchmen twelve issue comic book limited serie...\n",
       "1   en  wikipedia  citigroup center formerly citicorp center tall...\n",
       "2   en  wikipedia  birth_place death_date death_place party conse...\n",
       "3   en  wikipedia  marbod maroboduus born died king marcomanni no...\n",
       "4   en  wikipedia  sylvester medal bronze medal awarded every yea..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=['text'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: so as to work with the Jupyter Notebooks without repeating the same processes over and over we will just dump it into a JSON-Lines (.jsonl) file which will contain the `pandas.DataFrame` as a JSON object on each line of the file; but due to GitHub quotas and limits this file has been included in the .gitignore, so you will not be able to see it. Otherwise, just run this Jupyter Notebook in order to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(path_or_buf='PreProcessedDocuments.jsonl', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
